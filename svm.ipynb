{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "svm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ4o8HhmN-PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c695c1d1-b253-4389-dc84-fa7e610b6a97"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/11/4d/378ab91284c2c3a06ab475b287721c09b7951d5ecb3edf4ffb0e1e7a568a/contractions-0.0.49-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 33.9MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 39.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85394 sha256=32e1801b5e87ae01bb9f10d53f2ffc1b0cf6c558f94b9709724052aa55d2fbbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.49 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHcvtA5dOAkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd69be3-d998-47ca-ac93-9eb67930e920"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5ACchOBOCsp"
      },
      "source": [
        "#อ่าน csv file \n",
        "def read_csv(filename):  \n",
        "\n",
        "  csv_file = []\n",
        "\n",
        "  df = pd.read_csv(filename)\n",
        "  for index, row in df.iterrows():\n",
        "    csv_file.append((row['text'],row['category']))\n",
        "    \n",
        "    \n",
        "\n",
        "  data=pd.DataFrame(csv_file,columns=[\"text\",\"category\"])\n",
        "  return data \n",
        "\n",
        "\n",
        "\n",
        "  def listToString(s):  \n",
        "    \n",
        "    # initialize an empty string \n",
        "    str1 = \" \" \n",
        "    # return string   \n",
        "    return (str1.join(s)) \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0q80P5BOEVg"
      },
      "source": [
        "# remove punctuation\n",
        "rm_punctuation = lambda x: x.translate(str.maketrans('', '', string.punctuation + \"\\'\\n\\r\\t\"))\n",
        "\n",
        "\n",
        "# remove stopwords for english\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# rm_stopwords = lambda x: ' '.join([word for word in x.split() if word not in stop_words])\n",
        "rm_number = lambda x: ' '.join([i for i in x.split() if not i.isdigit()])\n",
        "\n",
        "def listToString(s):  \n",
        "    \n",
        "    # initialize an empty string \n",
        "    str1 = \" \" \n",
        "    # return string   \n",
        "    return (str1.join(s)) \n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    filtered_text =listToString(filtered_text)\n",
        "    return filtered_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def rm_NonEnglishWordsFunct(x):\n",
        "    words = set(nltk.corpus.words.words())\n",
        "    filteredSentence = \" \".join(w for w in nltk.wordpunct_tokenize(x) \\\n",
        "                                if w.lower() in words or not w.isalpha())\n",
        "    return filteredSentence\n",
        "\n",
        "def rm_Emoji(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "def rm_url(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def rm_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "# they're --> they are \n",
        "# def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "#     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "#                                       flags=re.IGNORECASE|re.DOTALL)\n",
        "#     def expand_match(contraction):\n",
        "#         match = contraction.group(0)\n",
        "#         first_char = match[0]\n",
        "#         expanded_contraction = contraction_mapping.get(match)\\\n",
        "#                                 if contraction_mapping.get(match)\\\n",
        "#                                 else contraction_mapping.get(match.lower())                       \n",
        "#         expanded_contraction = first_char+expanded_contraction[1:]\n",
        "#         return expanded_contraction\n",
        "        \n",
        "#     expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "#     expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "  \n",
        "#     return expanded_text\n",
        "\n",
        "def rm_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# preprocess text\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # text = rm_NonEnglishWordsFunct(text)\n",
        "    \n",
        "    # text = expand_contractions(text)\n",
        "    text = rm_url(text)\n",
        "    text = rm_html(text)\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    text = rm_punctuation(text)\n",
        "    # text = expand_contractions(text)\n",
        "    # text = rm_special_characters(text,remove_digits=True)\n",
        "   \n",
        "    # text = rm_stopwords(text)\n",
        "    text = rm_number(text)\n",
        "    text = rm_Emoji(text)\n",
        "    text = rm_special_characters(text,remove_digits=True)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "    \n",
        "  \n",
        "    # # text =rm_NonEnglishWordsFunct(text)\n",
        "   \n",
        "  \n",
        "    \n",
        "    \n",
        "    \n",
        "    # text= rm_number(text)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8rkYlGtOHSJ"
      },
      "source": [
        "def lemmatize_text(text):\n",
        "        to_wordnet_tag = defaultdict(lambda : wordnet.NOUN)\n",
        "        # to_wordnet_tag['N'] = wordnet.NOUN\n",
        "        to_wordnet_tag['V'] = wordnet.VERB\n",
        "        to_wordnet_tag['J'] = wordnet.ADJ\n",
        "        to_wordnet_tag['R'] = wordnet.ADV\n",
        "        \n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmata = [lemmatizer.lemmatize(word, pos=to_wordnet_tag[tag[0]]) for (word, tag) in text]\n",
        "\n",
        "  # แปลง lemmata ที่เป็น list เป็น string \n",
        "        lemmata =listToString(lemmata)\n",
        "        return lemmata\n",
        "\n",
        "\n",
        "    \n",
        "def preprocessing_pipeline(document):\n",
        "    \"\"\"\n",
        "    @params\n",
        "    - document: (n,1) one dimensional shaped series were each entry is a string\n",
        "    \n",
        "    @return\n",
        "    - return document with preprocessed columns\n",
        "    \"\"\"\n",
        "    df = document.to_frame()\n",
        "    print('Tokenize Text...')\n",
        "    df['tokens'] = document.apply(word_tokenize)\n",
        "    \n",
        "    print('Build Part of Speech Tags...')\n",
        "    df['pos'] = df.tokens.apply(pos_tag)\n",
        "    \n",
        "    print('Build Lemmata...')\n",
        "    df['lemma'] = df.pos.apply(lemmatize_text)\n",
        "\n",
        "    # df =listToString(df)\n",
        "    # df['lemma'] = ' '.join(map(str, df['lemma'])) \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FBpNngzOJAl"
      },
      "source": [
        "filename = 'data_training5.csv'\n",
        "input_data=read_csv(filename) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQP299ksOLDn"
      },
      "source": [
        "training_data =input_data.copy()\n",
        "training_data['text'] = training_data.text.apply(preprocess_text)\n",
        "# ลบ row ที่ว่าง \n",
        "training_data = training_data[training_data.text.apply(len) > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y74OUmITON6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "outputId": "6b288d9b-4838-425c-ebed-dee92377ac29"
      },
      "source": [
        "# preprocess text\n",
        "X_data = preprocessing_pipeline(training_data.text)\n",
        "X_data.head(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize Text...\n",
            "Build Part of Speech Tags...\n",
            "Build Lemmata...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>pos</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sony reality audio speaker srsrs first impress...</td>\n",
              "      <td>[sony, reality, audio, speaker, srsrs, first, ...</td>\n",
              "      <td>[(sony, NN), (reality, NN), (audio, NN), (spea...</td>\n",
              "      <td>sony reality audio speaker srsrs first impression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>realme pro review better camera iphone</td>\n",
              "      <td>[realme, pro, review, better, camera, iphone]</td>\n",
              "      <td>[(realme, JJ), (pro, FW), (review, FW), (bette...</td>\n",
              "      <td>realme pro review good camera iphone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>galaxy ultra magnetic case nudient v review</td>\n",
              "      <td>[galaxy, ultra, magnetic, case, nudient, v, re...</td>\n",
              "      <td>[(galaxy, NN), (ultra, JJ), (magnetic, JJ), (c...</td>\n",
              "      <td>galaxy ultra magnetic case nudient v review</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>best work home headphones microphone epos adap...</td>\n",
              "      <td>[best, work, home, headphones, microphone, epo...</td>\n",
              "      <td>[(best, JJS), (work, NN), (home, NN), (headpho...</td>\n",
              "      <td>best work home headphone microphone epos adapt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>realme pro camera test mp quad shooter</td>\n",
              "      <td>[realme, pro, camera, test, mp, quad, shooter]</td>\n",
              "      <td>[(realme, JJ), (pro, JJ), (camera, NN), (test,...</td>\n",
              "      <td>realme pro camera test mp quad shooter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>huawei p pro vs oppo find x pro one wins</td>\n",
              "      <td>[huawei, p, pro, vs, oppo, find, x, pro, one, ...</td>\n",
              "      <td>[(huawei, NN), (p, JJ), (pro, JJ), (vs, FW), (...</td>\n",
              "      <td>huawei p pro v oppo find x pro one win</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>huawei p pro moon zoom good x</td>\n",
              "      <td>[huawei, p, pro, moon, zoom, good, x]</td>\n",
              "      <td>[(huawei, NN), (p, JJ), (pro, JJ), (moon, NN),...</td>\n",
              "      <td>huawei p pro moon zoom good x</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>huawei gt e vs huawei gt difference</td>\n",
              "      <td>[huawei, gt, e, vs, huawei, gt, difference]</td>\n",
              "      <td>[(huawei, NN), (gt, NN), (e, NN), (vs, NN), (h...</td>\n",
              "      <td>huawei gt e v huawei gt difference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>sony xperia ultra could xperia ii plus new ultra</td>\n",
              "      <td>[sony, xperia, ultra, could, xperia, ii, plus,...</td>\n",
              "      <td>[(sony, NN), (xperia, NNS), (ultra, JJ), (coul...</td>\n",
              "      <td>sony xperia ultra could xperia ii plus new ultra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>sennheiser gsp unboxing review ps audiophile g...</td>\n",
              "      <td>[sennheiser, gsp, unboxing, review, ps, audiop...</td>\n",
              "      <td>[(sennheiser, NN), (gsp, NN), (unboxing, VBG),...</td>\n",
              "      <td>sennheiser gsp unbox review p audiophile game ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  ...                                              lemma\n",
              "0   sony reality audio speaker srsrs first impress...  ...  sony reality audio speaker srsrs first impression\n",
              "1              realme pro review better camera iphone  ...               realme pro review good camera iphone\n",
              "2         galaxy ultra magnetic case nudient v review  ...        galaxy ultra magnetic case nudient v review\n",
              "3   best work home headphones microphone epos adap...  ...  best work home headphone microphone epos adapt...\n",
              "4              realme pro camera test mp quad shooter  ...             realme pro camera test mp quad shooter\n",
              "..                                                ...  ...                                                ...\n",
              "95           huawei p pro vs oppo find x pro one wins  ...             huawei p pro v oppo find x pro one win\n",
              "96                      huawei p pro moon zoom good x  ...                      huawei p pro moon zoom good x\n",
              "97                huawei gt e vs huawei gt difference  ...                 huawei gt e v huawei gt difference\n",
              "98   sony xperia ultra could xperia ii plus new ultra  ...   sony xperia ultra could xperia ii plus new ultra\n",
              "99  sennheiser gsp unboxing review ps audiophile g...  ...  sennheiser gsp unbox review p audiophile game ...\n",
              "\n",
              "[100 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5odnuVlsOQT7"
      },
      "source": [
        "training_data_cleaned =pd.merge(X_data, training_data)\n",
        "\n",
        "training_data_cleaned.to_csv(\"cleaned_lemma_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB48JYTQOSOJ"
      },
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#GET VECTOR COUNT\n",
        "count_vect = CountVectorizer(analyzer='word', ngram_range=(1,2))\n",
        "X_train_counts = count_vect.fit_transform(training_data_cleaned.lemma) \n",
        "# print(count_vect.get_feature_names())\n",
        "\n",
        "#SAVE WORD VECTOR\n",
        "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))\n",
        "pickle.dump(count_vect.get_feature_names(), open(\"count_vector.txt\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S6pWNuXOUuH"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#TRANSFORM WORD VECTOR TO TF IDF\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "\n",
        "#SAVE TF-IDF\n",
        "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e9HziLjOWhX"
      },
      "source": [
        "test_size = 0.20 \n",
        "random_statee  = 42 \n",
        "cv=5 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3d3sU_TOYrj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd18b3f-f784-4b09-ac87-3ce50f48acf9"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score \n",
        "# SVM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "clf_svm = svm.LinearSVC()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data_cleaned.category, test_size=test_size, random_state=random_statee)\n",
        "# clf_svm.fit(X_train_tfidf, training_data_cleaned.category)\n",
        "clf_svm.fit(X_train, y_train)\n",
        "# score =cross_val_score(clf_svm,X_train,y_train ,cv=5,scoring=\"accuracy\")\n",
        "score =cross_val_score(clf_svm,X_train,y_train ,cv=10,scoring=\"accuracy\")\n",
        "print(score)\n",
        "# print(score.mean())\n",
        "# print(score2)\n",
        "# print(score2.mean())\n",
        "# pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.80535456 0.78473227 0.80680174 0.79848046 0.80824891 0.81295224\n",
            " 0.80166486 0.80275063 0.7922548  0.79297865]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDca5MKiObLQ"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
        "\n",
        "def classification_report_with_accuracy_score(y_true, y_pred):\n",
        "\n",
        "    print (classification_report(y_true, y_pred)) # print classification report\n",
        "    return accuracy_score(y_true, y_pred) # return accuracy score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzq05LrmOp3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878ab363-eb05-4dda-ecb9-e984f06de840"
      },
      "source": [
        "# Nested CV with parameter optimization\n",
        "nested_score = cross_val_score(clf_svm,X_train,y_train,cv=10, \\\n",
        "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
        "print (nested_score) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.70      0.82       291\n",
            "           1       0.99      0.80      0.88       315\n",
            "           2       0.52      0.84      0.64       259\n",
            "           3       0.38      0.95      0.55       239\n",
            "           4       0.97      0.79      0.87       257\n",
            "           5       1.00      0.79      0.88       288\n",
            "           6       0.87      0.60      0.71       249\n",
            "           7       0.99      0.79      0.88       359\n",
            "           8       0.90      0.77      0.83       260\n",
            "           9       1.00      0.72      0.84       247\n",
            "\n",
            "    accuracy                           0.77      2764\n",
            "   macro avg       0.86      0.77      0.79      2764\n",
            "weighted avg       0.87      0.77      0.80      2764\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.74      0.85       291\n",
            "           1       0.98      0.82      0.89       315\n",
            "           2       0.55      0.90      0.68       259\n",
            "           3       0.37      0.92      0.53       239\n",
            "           4       0.93      0.73      0.82       257\n",
            "           5       1.00      0.78      0.88       288\n",
            "           6       0.88      0.59      0.71       249\n",
            "           7       1.00      0.73      0.84       359\n",
            "           8       0.90      0.78      0.84       260\n",
            "           9       1.00      0.75      0.86       247\n",
            "\n",
            "    accuracy                           0.77      2764\n",
            "   macro avg       0.86      0.77      0.79      2764\n",
            "weighted avg       0.87      0.77      0.80      2764\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.81      0.89       291\n",
            "           1       0.98      0.84      0.90       315\n",
            "           2       0.65      0.83      0.73       258\n",
            "           3       0.37      0.97      0.53       239\n",
            "           4       0.94      0.79      0.86       257\n",
            "           5       1.00      0.84      0.91       289\n",
            "           6       0.89      0.62      0.73       249\n",
            "           7       1.00      0.80      0.89       359\n",
            "           8       0.95      0.72      0.82       260\n",
            "           9       0.99      0.73      0.84       247\n",
            "\n",
            "    accuracy                           0.80      2764\n",
            "   macro avg       0.87      0.79      0.81      2764\n",
            "weighted avg       0.89      0.80      0.82      2764\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.76      0.86       292\n",
            "           1       0.99      0.79      0.88       315\n",
            "           2       0.55      0.83      0.66       258\n",
            "           3       0.37      0.93      0.53       239\n",
            "           4       0.95      0.72      0.82       256\n",
            "           5       1.00      0.78      0.88       289\n",
            "           6       0.85      0.62      0.72       250\n",
            "           7       1.00      0.79      0.88       359\n",
            "           8       0.92      0.79      0.85       259\n",
            "           9       0.99      0.77      0.86       247\n",
            "\n",
            "    accuracy                           0.78      2764\n",
            "   macro avg       0.86      0.78      0.79      2764\n",
            "weighted avg       0.87      0.78      0.80      2764\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.78      0.87       292\n",
            "           1       0.98      0.77      0.86       314\n",
            "           2       0.55      0.87      0.67       258\n",
            "           3       0.37      0.92      0.53       239\n",
            "           4       0.97      0.76      0.86       256\n",
            "           5       1.00      0.79      0.88       289\n",
            "           6       0.90      0.60      0.72       250\n",
            "           7       1.00      0.75      0.85       360\n",
            "           8       0.92      0.85      0.89       259\n",
            "           9       0.99      0.72      0.83       247\n",
            "\n",
            "    accuracy                           0.78      2764\n",
            "   macro avg       0.87      0.78      0.80      2764\n",
            "weighted avg       0.88      0.78      0.80      2764\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.77      0.86       292\n",
            "           1       0.97      0.82      0.89       315\n",
            "           2       0.70      0.83      0.76       258\n",
            "           3       0.34      0.97      0.50       240\n",
            "           4       0.94      0.71      0.81       256\n",
            "           5       1.00      0.82      0.90       289\n",
            "           6       0.95      0.58      0.72       249\n",
            "           7       0.99      0.81      0.89       360\n",
            "           8       0.91      0.81      0.86       259\n",
            "           9       1.00      0.69      0.81       246\n",
            "\n",
            "    accuracy                           0.78      2764\n",
            "   macro avg       0.88      0.78      0.80      2764\n",
            "weighted avg       0.89      0.78      0.81      2764\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.77      0.87       292\n",
            "           1       0.98      0.78      0.87       315\n",
            "           2       0.54      0.85      0.66       258\n",
            "           3       0.36      0.91      0.51       239\n",
            "           4       0.92      0.75      0.82       256\n",
            "           5       1.00      0.78      0.88       289\n",
            "           6       0.89      0.61      0.73       249\n",
            "           7       0.99      0.79      0.88       360\n",
            "           8       0.92      0.74      0.82       259\n",
            "           9       0.99      0.70      0.82       246\n",
            "\n",
            "    accuracy                           0.77      2763\n",
            "   macro avg       0.86      0.77      0.79      2763\n",
            "weighted avg       0.87      0.77      0.80      2763\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.76      0.86       292\n",
            "           1       0.98      0.82      0.89       315\n",
            "           2       0.57      0.85      0.68       258\n",
            "           3       0.38      0.94      0.54       239\n",
            "           4       0.96      0.75      0.84       256\n",
            "           5       1.00      0.79      0.88       289\n",
            "           6       0.90      0.60      0.72       249\n",
            "           7       1.00      0.80      0.89       360\n",
            "           8       0.88      0.79      0.83       259\n",
            "           9       0.99      0.72      0.84       246\n",
            "\n",
            "    accuracy                           0.78      2763\n",
            "   macro avg       0.86      0.78      0.80      2763\n",
            "weighted avg       0.88      0.78      0.81      2763\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.76      0.85       291\n",
            "           1       0.98      0.81      0.89       315\n",
            "           2       0.61      0.83      0.70       258\n",
            "           3       0.33      0.94      0.49       239\n",
            "           4       0.92      0.72      0.81       257\n",
            "           5       0.99      0.77      0.87       289\n",
            "           6       0.89      0.53      0.66       249\n",
            "           7       1.00      0.79      0.88       360\n",
            "           8       0.92      0.77      0.84       259\n",
            "           9       0.99      0.71      0.83       246\n",
            "\n",
            "    accuracy                           0.76      2763\n",
            "   macro avg       0.86      0.76      0.78      2763\n",
            "weighted avg       0.87      0.76      0.79      2763\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.73      0.84       291\n",
            "           1       0.98      0.75      0.85       315\n",
            "           2       0.55      0.87      0.67       258\n",
            "           3       0.36      0.93      0.52       239\n",
            "           4       0.96      0.75      0.84       257\n",
            "           5       1.00      0.77      0.87       289\n",
            "           6       0.91      0.62      0.74       249\n",
            "           7       0.99      0.79      0.88       359\n",
            "           8       0.92      0.76      0.83       259\n",
            "           9       0.99      0.72      0.84       247\n",
            "\n",
            "    accuracy                           0.77      2763\n",
            "   macro avg       0.87      0.77      0.79      2763\n",
            "weighted avg       0.88      0.77      0.80      2763\n",
            "\n",
            "[0.77496382 0.77279305 0.79558611 0.77785818 0.77930535 0.78364689\n",
            " 0.76981542 0.78248281 0.76221498 0.77017734]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju7gtWg20JKS",
        "outputId": "94dc50a5-5f8b-4e21-d0d3-013c46ab8d12"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score \n",
        "# SVM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "clf_svm = svm.SVC(kernel='poly')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data_cleaned.category, test_size=test_size, random_state=random_statee)\n",
        "# clf_svm.fit(X_train_tfidf, training_data_cleaned.category)\n",
        "clf_svm.fit(X_train, y_train)\n",
        "# score =cross_val_score(clf_svm,X_train,y_train ,cv=5,scoring=\"accuracy\")\n",
        "score =cross_val_score(clf_svm,X_train,y_train ,cv=10,scoring=\"accuracy\")\n",
        "print(score)\n",
        "# print(score.mean())\n",
        "# print(score2)\n",
        "# print(score2.mean())\n",
        "# pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.77496382 0.77279305 0.79558611 0.77785818 0.77930535 0.78364689\n",
            " 0.76981542 0.78248281 0.76221498 0.77017734]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD2LhO5G0JDN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}